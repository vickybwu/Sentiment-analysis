{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artificial Intelligence Final Code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_8zqzPvkLe19",
        "oQhGx_p5LZMt",
        "1i5JLI64MCya",
        "Zn6gNlYqKvTT",
        "DCj_gbgHIMv4",
        "SRiCtCLtQF1d",
        "z0YNZqJvT08F",
        "0V_1iRMVBZQd",
        "wJeqpG8Pa7Im"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhGx_p5LZMt"
      },
      "source": [
        "## **Data Prepration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvm3h4vDSku3"
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb9FjnG9Gzdh"
      },
      "source": [
        "## Read in the dataset\n",
        "import pandas as pd\n",
        "link = \"https://github.com/vickybwu/Myfiles/blob/main/Womens%20Clothing%20E-Commerce%20Reviews.csv?raw=true\"\n",
        "clothing_store = pd.read_csv(link)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "3yc2ZjYuG65i",
        "outputId": "e94022b9-2962-4fae-ceb6-bf6a97dab394"
      },
      "source": [
        "clothing_store.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Clothing ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Title</th>\n",
              "      <th>Review Text</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Recommended IND</th>\n",
              "      <th>Positive Feedback Count</th>\n",
              "      <th>Division Name</th>\n",
              "      <th>Department Name</th>\n",
              "      <th>Class Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>767</td>\n",
              "      <td>33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Initmates</td>\n",
              "      <td>Intimate</td>\n",
              "      <td>Intimates</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1080</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1077</td>\n",
              "      <td>60</td>\n",
              "      <td>Some major design flaws</td>\n",
              "      <td>I had such high hopes for this dress and reall...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1049</td>\n",
              "      <td>50</td>\n",
              "      <td>My favorite buy!</td>\n",
              "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>General Petite</td>\n",
              "      <td>Bottoms</td>\n",
              "      <td>Pants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>847</td>\n",
              "      <td>47</td>\n",
              "      <td>Flattering shirt</td>\n",
              "      <td>This shirt is very flattering to all due to th...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>General</td>\n",
              "      <td>Tops</td>\n",
              "      <td>Blouses</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Clothing ID  Age  ...   Division Name Department Name  Class Name\n",
              "0           0          767   33  ...       Initmates        Intimate   Intimates\n",
              "1           1         1080   34  ...         General         Dresses     Dresses\n",
              "2           2         1077   60  ...         General         Dresses     Dresses\n",
              "3           3         1049   50  ...  General Petite         Bottoms       Pants\n",
              "4           4          847   47  ...         General            Tops     Blouses\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcWq_kS2HEqy"
      },
      "source": [
        "## Only using two columns\n",
        "my_data = clothing_store[['Review Text', 'Recommended IND']].dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ue_fSm04IRNz",
        "outputId": "ec9561e6-1c69-42a5-c961-ea9edc7d082f"
      },
      "source": [
        "## Randomly Select n data points from each class\n",
        "## Let n be 1000\n",
        "import random\n",
        "\n",
        "recommended_reviews = my_data[my_data['Recommended IND'] == 1]\n",
        "not_recommended_reviews = my_data[my_data['Recommended IND'] == 0]\n",
        "\n",
        "indexes_1 = random.sample(range(0, len(recommended_reviews)), 1000)\n",
        "indexes_2 = random.sample(range(0, len(not_recommended_reviews)), 1000)\n",
        "\n",
        "recommended_reviews = recommended_reviews.iloc[indexes_1]\n",
        "not_recommended_reviews = not_recommended_reviews.iloc[indexes_2]\n",
        "\n",
        "selected = recommended_reviews.append(not_recommended_reviews, ignore_index = True)\n",
        "\n",
        "# shuffle the DataFrame rows \n",
        "selected = selected.sample(frac = 1) \n",
        "selected = selected.reset_index(drop=True)\n",
        "\n",
        "selected.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review Text</th>\n",
              "      <th>Recommended IND</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Overall a very nice and unique dress. it does ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this dress so much! i bought the cranberr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great peplum top, beautiful whine color. very ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I love this tunic! with that said the reason i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This sweater dress color is great and texture ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Review Text  Recommended IND\n",
              "0  Overall a very nice and unique dress. it does ...                1\n",
              "1  Love this dress so much! i bought the cranberr...                1\n",
              "2  Great peplum top, beautiful whine color. very ...                1\n",
              "3  I love this tunic! with that said the reason i...                1\n",
              "4  This sweater dress color is great and texture ...                0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C7rviOjrXH_",
        "outputId": "8873a97d-958a-4eca-ea02-d1d838915d81"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zTv2BD4HRazW",
        "outputId": "2152358b-4764-4c1f-b071-a559fad5fe01"
      },
      "source": [
        "## Covert reviews into bag of words \n",
        "\n",
        "# convert to lowercase string \n",
        "selected['Cleaned_Text'] = [str(review) for review in selected['Review Text']]\n",
        "selected['Cleaned_Text'] = [review.lower() for review in selected['Cleaned_Text']]\n",
        "# Strip all punctuation and numeric values from each review\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "selected['Cleaned_Text'] = [review.translate(table) for review in selected['Cleaned_Text']]\n",
        "#  Tokenization : In this each entry in the corpus will be broken into set of words\n",
        "selected['Cleaned_Text'] = [word_tokenize(review) for review in selected['Cleaned_Text']]\n",
        "\n",
        "selected.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review Text</th>\n",
              "      <th>Recommended IND</th>\n",
              "      <th>Cleaned_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Overall a very nice and unique dress. it does ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[overall, a, very, nice, and, unique, dress, i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this dress so much! i bought the cranberr...</td>\n",
              "      <td>1</td>\n",
              "      <td>[love, this, dress, so, much, i, bought, the, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great peplum top, beautiful whine color. very ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[great, peplum, top, beautiful, whine, color, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I love this tunic! with that said the reason i...</td>\n",
              "      <td>1</td>\n",
              "      <td>[i, love, this, tunic, with, that, said, the, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This sweater dress color is great and texture ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[this, sweater, dress, color, is, great, and, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Review Text  ...                                       Cleaned_Text\n",
              "0  Overall a very nice and unique dress. it does ...  ...  [overall, a, very, nice, and, unique, dress, i...\n",
              "1  Love this dress so much! i bought the cranberr...  ...  [love, this, dress, so, much, i, bought, the, ...\n",
              "2  Great peplum top, beautiful whine color. very ...  ...  [great, peplum, top, beautiful, whine, color, ...\n",
              "3  I love this tunic! with that said the reason i...  ...  [i, love, this, tunic, with, that, said, the, ...\n",
              "4  This sweater dress color is great and texture ...  ...  [this, sweater, dress, color, is, great, and, ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wGc65Si0TKId",
        "outputId": "7eb59eb3-828a-4c73-d4e4-3fc7377901c8"
      },
      "source": [
        "## Remove words with less semantic meaning and lemmatize\n",
        "\n",
        "# create tag map  (code took from https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python)\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "# Remove Stop words, Non-Numeric and perfom word Lemmenting.\n",
        "for index,review in enumerate(selected['Cleaned_Text']):\n",
        "    Final_words = []\n",
        "    \n",
        "    Word_Lemmatized = WordNetLemmatizer()\n",
        "    # identify if the word is Noun(N) or Verb(V) or something else using post tag\n",
        "    for word, tag in pos_tag(review):\n",
        "        # remove stop words\n",
        "        if word not in stopwords.words('english') and word.isalpha():\n",
        "            word_Final = Word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "            Final_words.append(word_Final)\n",
        "    # The final processed set of words for each iteration will be stored in 'cleaned_text'(in the format of lists) and 'text_final'(in the format of strings)\n",
        "    selected.at[index,'Cleaned_Text'] = Final_words\n",
        "    selected.loc[index, 'Text_Final'] = str(Final_words)\n",
        "    \n",
        "selected.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review Text</th>\n",
              "      <th>Recommended IND</th>\n",
              "      <th>Cleaned_Text</th>\n",
              "      <th>Text_Final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Overall a very nice and unique dress. it does ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[overall, nice, unique, dress, much, go, wear,...</td>\n",
              "      <td>['overall', 'nice', 'unique', 'dress', 'much',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this dress so much! i bought the cranberr...</td>\n",
              "      <td>1</td>\n",
              "      <td>[love, dress, much, buy, cranberry, color, fir...</td>\n",
              "      <td>['love', 'dress', 'much', 'buy', 'cranberry', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great peplum top, beautiful whine color. very ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[great, peplum, top, beautiful, whine, color, ...</td>\n",
              "      <td>['great', 'peplum', 'top', 'beautiful', 'whine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I love this tunic! with that said the reason i...</td>\n",
              "      <td>1</td>\n",
              "      <td>[love, tunic, say, reason, give, star, materia...</td>\n",
              "      <td>['love', 'tunic', 'say', 'reason', 'give', 'st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This sweater dress color is great and texture ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[sweater, dress, color, great, texture, nice, ...</td>\n",
              "      <td>['sweater', 'dress', 'color', 'great', 'textur...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Review Text  ...                                         Text_Final\n",
              "0  Overall a very nice and unique dress. it does ...  ...  ['overall', 'nice', 'unique', 'dress', 'much',...\n",
              "1  Love this dress so much! i bought the cranberr...  ...  ['love', 'dress', 'much', 'buy', 'cranberry', ...\n",
              "2  Great peplum top, beautiful whine color. very ...  ...  ['great', 'peplum', 'top', 'beautiful', 'whine...\n",
              "3  I love this tunic! with that said the reason i...  ...  ['love', 'tunic', 'say', 'reason', 'give', 'st...\n",
              "4  This sweater dress color is great and texture ...  ...  ['sweater', 'dress', 'color', 'great', 'textur...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i5JLI64MCya"
      },
      "source": [
        "## **Multinomial Naive Bayes Model (built from scratch) with Bog of Words approach**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_YwTcDzI6M5"
      },
      "source": [
        "## P(c)    \n",
        "def Priors(Train_set):\n",
        "    priors = {}\n",
        "    \n",
        "    is_not_recommended = Train_set[\"Recommended IND\"] == 0\n",
        "    Not_Recommended = Train_set[is_not_recommended]\n",
        "    \n",
        "    recommended = Train_set[\"Recommended IND\"] == 1\n",
        "    Recommended = Train_set[recommended]\n",
        "    \n",
        "    priors['Recommended'] = len(Recommended)/len(Train_set)\n",
        "    priors['Not_Recommended'] = len(Not_Recommended)/len(Train_set)\n",
        "    \n",
        "    return priors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE2ENzHdLEFg"
      },
      "source": [
        "## P(d|c)\n",
        "from collections import Counter\n",
        "\n",
        "def Likelihood(Train_set, evidence): # Given one piece of evidence\n",
        "    likelihood = {}\n",
        "\n",
        "    # Seperate the training set into Recommended and Not Recommended subsets\n",
        "    is_recommended = Train_set[\"Recommended IND\"] == 1\n",
        "    Recommended = Train_set[is_recommended]\n",
        "    \n",
        "    is_not_recommended = Train_set[\"Recommended IND\"] == 0\n",
        "    Not_Recommended = Train_set[is_not_recommended]\n",
        "\n",
        "    \n",
        "    # Create dictionary for the R subset - Word: frequency\n",
        "\n",
        "    wordlistR = []\n",
        "    for review in Recommended['Cleaned_Text']:\n",
        "        for word in review:\n",
        "          wordlistR.append(str(word))\n",
        "\n",
        "    FrequencyR = Counter(wordlistR)\n",
        "\n",
        "    total_count_r = sum(FrequencyR.values())\n",
        "    \n",
        "   #print(FrequencyR)\n",
        "\n",
        "    \n",
        "    # Create dictionary for the NR subset - Word: frequency\n",
        "    wordlistNR = []\n",
        "    for review in Not_Recommended['Cleaned_Text']:\n",
        "        for word in review:\n",
        "          wordlistNR.append(str(word))\n",
        "\n",
        "    FrequencyNR = Counter(wordlistNR)\n",
        "\n",
        "    total_count_nr = sum(FrequencyNR.values())\n",
        "    \n",
        "    #print(FrequencyNR)\n",
        "\n",
        "    # For \"Recommended\" Class, calculate the likelihood of each word and multiply them, to avoid underflow, we add 1 to each word count\n",
        "    Probability_R = 1\n",
        "    for word in evidence:\n",
        "        probs_r = (FrequencyR[word] + 1)/total_count_r\n",
        "        #print(probs_r)\n",
        "        Probability_R *= probs_r \n",
        "        \n",
        "    # For \"Not_Recommended\" Class, calculate the likelihood of each word and multiply them, to avoid underflow, we add 1 to each word count\n",
        "    Probability_NR = 1\n",
        "    for word in evidence:\n",
        "        probs_nr = (FrequencyNR[word] + 1)/total_count_nr\n",
        "        Probability_NR *= probs_nr \n",
        "        \n",
        "    likelihood[\"Recommended\"] = Probability_R\n",
        "    likelihood[\"Not_Recommended\"] = Probability_NR\n",
        "    \n",
        "    return likelihood\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV2Y6umlLEyB"
      },
      "source": [
        "def Posteriors(Train_set, evidence): # Given one piece of evidence\n",
        "    posteriors = {}\n",
        "    \n",
        "    # For recommended \n",
        "    prior_recommended = Priors(Train_set)['Recommended']\n",
        "    likelihood_recommended = Likelihood(Train_set, evidence)['Recommended']\n",
        "    posterior_recommended = prior_recommended * likelihood_recommended\n",
        "    \n",
        "    # For not recommended\n",
        "    prior_unrecommended = Priors(Train_set)['Not_Recommended']\n",
        "    likelihood_unrecommended = Likelihood(Train_set, evidence)['Not_Recommended']\n",
        "    posterior_unrecommended = prior_unrecommended * likelihood_unrecommended\n",
        "    \n",
        "    posteriors['Recommended'] = posterior_recommended\n",
        "    posteriors['Not_Recommended'] = posterior_unrecommended\n",
        "    \n",
        "    return posteriors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk4FT-HMLJHQ"
      },
      "source": [
        "def decision_rule(Train_set, evidence):\n",
        "    posteriors = Posteriors(Train_set, evidence)\n",
        "    decision_class =  max(posteriors, key = posteriors.get)\n",
        "    return decision_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdlyZ2i1sItU"
      },
      "source": [
        "def evaluate_accuracy(Train_set, Test_set):\n",
        "    y_true = []\n",
        "    for x in Test_set['Recommended IND']:\n",
        "        if x == 1:\n",
        "            y_true.append('Recommended')\n",
        "        if x == 0:\n",
        "            y_true.append('Not_Recommended')\n",
        "    #print(y_true)\n",
        "        \n",
        "    y_predicted = []\n",
        "    for evidence in Test_set['Cleaned_Text']:\n",
        "        prediction = decision_rule(Train_set, evidence)\n",
        "        y_predicted.append(prediction)\n",
        "    #print(y_predicted)\n",
        "        \n",
        "    \n",
        "    accuracy = 0\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i] == y_predicted[i]:\n",
        "            accuracy += 1\n",
        "            \n",
        "    return accuracy/float(len(y_true))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C7Wabwhr2L4"
      },
      "source": [
        "## Train-Set split of 80-20\n",
        "\n",
        "test_size = int(len(selected)*0.2)\n",
        "\n",
        "indexes = random.sample(range(0, len(selected)), test_size)\n",
        "\n",
        "Test_set = selected.iloc[indexes]\n",
        "\n",
        "Train_set = selected.drop(indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfNYp39JAk2Q",
        "outputId": "d97f9a1c-2604-4363-d221-19d138f61d3c"
      },
      "source": [
        "accuracy_score = evaluate_accuracy(Train_set, Test_set)\n",
        "\n",
        "print(\"The accuracy rate from the test set is\", accuracy_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy rate from the test set is 0.8575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_-Nor7UIBDJ"
      },
      "source": [
        "# **Using SKlearn Models: Naive Bayes, KMeans, Logistic Regression, SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn6gNlYqKvTT"
      },
      "source": [
        "## **TFIDF Score Vectorization, no scaling, no dimension reduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duvbiW5Pxo45"
      },
      "source": [
        "Train_Test_Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOhn82HTtSDW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train,y_test = train_test_split(selected['Text_Final'], selected['Recommended IND'], test_size = 0.2,\n",
        "                                                   random_state = 25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKeQviGpBCSA"
      },
      "source": [
        "##  TFIDF Vectorization\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Vectorization using tf-idf scores\n",
        "# There are only less than 5000 features (words) in the dataset, set max_feature to 5000 to include all the features\n",
        "tfidfvector = TfidfVectorizer(max_features = 5000)\n",
        "tfidfvector.fit(selected['Text_Final'])\n",
        "TrainX_transformed = tfidfvector.transform(x_train)\n",
        "TestX_transformed = tfidfvector.transform(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "MHGGRn-YhUar",
        "outputId": "f184bc3d-dcb5-47d4-f2b9-48f57712c699"
      },
      "source": [
        "tfidf_df = pd.DataFrame(TrainX_transformed.toarray(), columns = tfidfvector.get_feature_names())\n",
        "tfidf_df\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aaaaannnnnnd</th>\n",
              "      <th>aaahed</th>\n",
              "      <th>abdomen</th>\n",
              "      <th>able</th>\n",
              "      <th>abovetheknee</th>\n",
              "      <th>absolute</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abt</th>\n",
              "      <th>abundance</th>\n",
              "      <th>abut</th>\n",
              "      <th>accent</th>\n",
              "      <th>accentuate</th>\n",
              "      <th>accentuated</th>\n",
              "      <th>accentuates</th>\n",
              "      <th>accessorize</th>\n",
              "      <th>accessorizes</th>\n",
              "      <th>accessory</th>\n",
              "      <th>accidentally</th>\n",
              "      <th>acco</th>\n",
              "      <th>accommodates</th>\n",
              "      <th>accompany</th>\n",
              "      <th>accord</th>\n",
              "      <th>accordinglythis</th>\n",
              "      <th>accurate</th>\n",
              "      <th>accurately</th>\n",
              "      <th>achieve</th>\n",
              "      <th>across</th>\n",
              "      <th>acrylic</th>\n",
              "      <th>acrylicwool</th>\n",
              "      <th>act</th>\n",
              "      <th>active</th>\n",
              "      <th>actual</th>\n",
              "      <th>actuality</th>\n",
              "      <th>actually</th>\n",
              "      <th>add</th>\n",
              "      <th>added</th>\n",
              "      <th>addict</th>\n",
              "      <th>addition</th>\n",
              "      <th>...</th>\n",
              "      <th>xspwas</th>\n",
              "      <th>xtr</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxl</th>\n",
              "      <th>xxs</th>\n",
              "      <th>xxsit</th>\n",
              "      <th>xxsmall</th>\n",
              "      <th>xxsp</th>\n",
              "      <th>yak</th>\n",
              "      <th>yarn</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yellowbeige</th>\n",
              "      <th>yellowed</th>\n",
              "      <th>yellowish</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>yetthough</th>\n",
              "      <th>yield</th>\n",
              "      <th>yikes</th>\n",
              "      <th>yoga</th>\n",
              "      <th>yoke</th>\n",
              "      <th>yoself</th>\n",
              "      <th>youd</th>\n",
              "      <th>youll</th>\n",
              "      <th>young</th>\n",
              "      <th>youre</th>\n",
              "      <th>youthfull</th>\n",
              "      <th>youve</th>\n",
              "      <th>yr</th>\n",
              "      <th>yuk</th>\n",
              "      <th>zero</th>\n",
              "      <th>zip</th>\n",
              "      <th>zippedfor</th>\n",
              "      <th>zipper</th>\n",
              "      <th>zoolanders</th>\n",
              "      <th>zuma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.212044</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.155333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.16981</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600 rows × 4369 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       aa  aaaaannnnnnd  aaahed  abdomen  ...  zippedfor    zipper  zoolanders  zuma\n",
              "0     0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "1     0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "2     0.0           0.0     0.0      0.0  ...        0.0  0.155333         0.0   0.0\n",
              "3     0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "4     0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "...   ...           ...     ...      ...  ...        ...       ...         ...   ...\n",
              "1595  0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "1596  0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "1597  0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "1598  0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "1599  0.0           0.0     0.0      0.0  ...        0.0  0.000000         0.0   0.0\n",
              "\n",
              "[1600 rows x 4369 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXIVbmiGFyI"
      },
      "source": [
        "**Classification using different models on unscaled TFIDF scores without dimension reduction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwedT8wu7ef0",
        "outputId": "1b4cc880-6293-4410-b9c6-8bc8b0ad3902"
      },
      "source": [
        "## Using K Means Clustering from sklearn without PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans_model = KMeans(n_clusters = 2)\n",
        "kmeans_model.fit(TrainX_transformed,y_train)\n",
        "prediction = kmeans_model.predict(TestX_transformed)\n",
        "accuracy = accuracy_score(prediction, y_test)\n",
        "print(\"KMeans produced an accuracy score of\", accuracy, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KMeans produced an accuracy score of 0.495 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxgI7rOiwqq0",
        "outputId": "893a02d2-3ac8-462a-f694-8f2cbc7174f7"
      },
      "source": [
        "## Using SVM from sklearn without PCA\n",
        "svm_model = svm.SVC(kernel = 'rbf', gamma = 'scale')\n",
        "\n",
        "svm_model.fit(TrainX_transformed, y_train)\n",
        "\n",
        "score = svm_model.score(TestX_transformed, y_test)\n",
        "\n",
        "print(\"SVM produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM produced an accuracy score of 0.8525 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz5sQ_4CBZTT",
        "outputId": "b90ff96c-75e1-4b66-9b69-7d0c16015dde"
      },
      "source": [
        "## Using Logistic Regression without regularization without PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logit_model = LogisticRegression(penalty = 'none').fit(TrainX_transformed, y_train)\n",
        "score = logit_model.score(TestX_transformed, y_test)\n",
        "print(\"Logistic regression without penalty produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression without penalty produced an accuracy score of 0.7925 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aqfMti9Etjg",
        "outputId": "3d094d44-a816-4a7b-ac12-c7eec5ac4603"
      },
      "source": [
        "## Using Logistic Regression with L2 penalty wihtout PCA\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logit_model_2= LogisticRegression(penalty = 'l2').fit(TrainX_transformed, y_train)\n",
        "score = logit_model_2.score(TestX_transformed, y_test)\n",
        "print(\"Logistic regression produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression produced an accuracy score of 0.8525 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCj_gbgHIMv4"
      },
      "source": [
        "## **TFIDF Score Vectorization, Scaled between 0-1, no dimension reduction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP2qO57SIWfO"
      },
      "source": [
        "## Scale all the TFIDF scores to be in between 0 and 1\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaled_xtrain = MinMaxScaler().fit_transform(TrainX_transformed.toarray())\n",
        "scaled_xtest = MinMaxScaler().fit_transform(TestX_transformed.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW2O4Je7QMKo",
        "outputId": "6cdcdd96-07d5-4e13-e298-ef3a931bbb9e"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# The feature values are not normally distributed but trying the model \n",
        "\n",
        "gaussian_model = GaussianNB()\n",
        "gaussian_model.fit(scaled_xtrain, y_train)\n",
        "prediction = gaussian_model.predict(scaled_xtest)\n",
        "accuracy = accuracy_score(prediction, y_test)\n",
        "print(\"Gaussian Naive Bayes produced an accuracy score of\", accuracy, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes produced an accuracy score of 0.6625 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHunruEsJw-2",
        "outputId": "2ef39915-7aa8-4382-dd30-f0617c2711ba"
      },
      "source": [
        "## Using K Means Clustering from sklearn without PCA\n",
        "\n",
        "kmeans_model = KMeans(n_clusters = 2)\n",
        "kmeans_model.fit(scaled_xtrain,y_train)\n",
        "prediction = kmeans_model.predict(scaled_xtest)\n",
        "accuracy = accuracy_score(prediction, y_test)\n",
        "print(\"KMeans produced an accuracy score of\", accuracy, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KMeans produced an accuracy score of 0.33 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrXh6_QEJ_Nn",
        "outputId": "9de52c86-2287-459c-93d8-5ed53224a8ab"
      },
      "source": [
        "## Using SVM from sklearn without PCA\n",
        "svm_model = svm.SVC(kernel = 'rbf', gamma = 'scale')\n",
        "\n",
        "svm_model.fit(scaled_xtrain, y_train)\n",
        "\n",
        "score = svm_model.score(scaled_xtest, y_test)\n",
        "\n",
        "print(\"SVM produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM produced an accuracy score of 0.8125 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J8BYO_ZKIvc",
        "outputId": "2b3a1898-92a3-4a0d-8916-4714d738ae48"
      },
      "source": [
        "## Using Logistic Regression without regularization without PCA\n",
        "\n",
        "logit_model = LogisticRegression(penalty = 'none').fit(scaled_xtrain, y_train)\n",
        "score = logit_model.score(scaled_xtest, y_test)\n",
        "print(\"Logistic regression without penalty produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression without penalty produced an accuracy score of 0.785 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEmErY36KPEp",
        "outputId": "b80eff5b-480e-4ab3-c5df-f399a3292ac9"
      },
      "source": [
        "## Using Logistic Regression with L2 penalty wihtout PCA\n",
        "\n",
        "logit_model_2= LogisticRegression(penalty = 'l2').fit(scaled_xtrain, y_train)\n",
        "score = logit_model_2.score(scaled_xtest, y_test)\n",
        "print(\"Logistic regression produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression produced an accuracy score of 0.815 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRiCtCLtQF1d"
      },
      "source": [
        "## **TFIDF Score Vectoriaztion, Data Scaling, PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElsJmScQRoSL"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Vectorization on original text data \n",
        "tfidfvector = TfidfVectorizer(max_features = 5000)\n",
        "tfidf_scores = tfidfvector.fit_transform(selected['Text_Final'])\n",
        "\n",
        "## Scale all the TFIDF scores to be in between 0 and 1 using MinMaxScaler before PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaled_scores = MinMaxScaler().fit_transform(tfidf_scores.toarray())\n",
        "\n",
        "# Reduce dimension using PCA to keep 95% explained variance \n",
        "pca = PCA(n_components = 0.95)\n",
        "\n",
        "reduced_tfidf = pca.fit_transform(scaled_scores)\n",
        "\n",
        "x_train, x_test, y_train,y_test = train_test_split(reduced_tfidf, selected['Recommended IND'], test_size = 0.2,\n",
        "                                                   random_state = 30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV3lpE9iTQuF",
        "outputId": "9f08c7f8-26b0-418f-a213-79e370b16378"
      },
      "source": [
        "# Examine the correlation coefficients between two random features\n",
        "# If the coefficient is too small PCA probably won't help with classification accuracy\n",
        "print(\"Pick two random features\")\n",
        "I =  np.random.randint(0, 4000, 50)\n",
        "for i in I:\n",
        "  x = tfidf_scores.toarray().T[i]\n",
        "  y = tfidf_scores.toarray().T[i+1]\n",
        "  r = np.corrcoef(x, y)\n",
        "  print(\"The coefficient between feature\", i, \"and feature\", i+1, \"is\", r[0,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pick two random features\n",
            "The coefficient between feature 1602 and feature 1603 is -0.0007075541938640144\n",
            "The coefficient between feature 2993 and feature 2994 is -0.0007064425904667866\n",
            "The coefficient between feature 3478 and feature 3479 is -0.0038105107133215924\n",
            "The coefficient between feature 818 and feature 819 is -0.0005002501250625325\n",
            "The coefficient between feature 3669 and feature 3670 is -0.0007023054485455137\n",
            "The coefficient between feature 3299 and feature 3300 is -0.012190476767178557\n",
            "The coefficient between feature 3704 and feature 3705 is -0.004975321708055041\n",
            "The coefficient between feature 1996 and feature 1997 is -0.0011968399214793038\n",
            "The coefficient between feature 1066 and feature 1067 is -0.00050025012506253\n",
            "The coefficient between feature 170 and feature 171 is -0.0005002501250625304\n",
            "The coefficient between feature 1536 and feature 1537 is -0.009206924035801532\n",
            "The coefficient between feature 3660 and feature 3661 is -0.0025968600040935617\n",
            "The coefficient between feature 486 and feature 487 is -0.002147761814040441\n",
            "The coefficient between feature 3130 and feature 3131 is -0.0015983570307495979\n",
            "The coefficient between feature 1088 and feature 1089 is -0.0006886996068514788\n",
            "The coefficient between feature 3197 and feature 3198 is -0.0005002501250625365\n",
            "The coefficient between feature 1364 and feature 1365 is -0.0008419976310960931\n",
            "The coefficient between feature 2724 and feature 2725 is -0.001303414639079041\n",
            "The coefficient between feature 3234 and feature 3235 is -0.0025917244908305005\n",
            "The coefficient between feature 3355 and feature 3356 is -0.0008607208884099556\n",
            "The coefficient between feature 2849 and feature 2850 is 0.032056119427374095\n",
            "The coefficient between feature 3337 and feature 3338 is -0.0007076313472081068\n",
            "The coefficient between feature 3796 and feature 3797 is -0.0009544898276195855\n",
            "The coefficient between feature 1711 and feature 1712 is -0.0035515675354387734\n",
            "The coefficient between feature 1312 and feature 1313 is -0.000500250125062527\n",
            "The coefficient between feature 1631 and feature 1632 is -0.0005002501250625255\n",
            "The coefficient between feature 1763 and feature 1764 is -0.0007032462814690678\n",
            "The coefficient between feature 1092 and feature 1093 is -0.0015922035221306778\n",
            "The coefficient between feature 2559 and feature 2560 is -0.009698572206962523\n",
            "The coefficient between feature 2053 and feature 2054 is -0.0012925331777494924\n",
            "The coefficient between feature 2192 and feature 2193 is -0.0010007990301256335\n",
            "The coefficient between feature 1277 and feature 1278 is -0.0005002501250625296\n",
            "The coefficient between feature 997 and feature 998 is -0.0019035934889537476\n",
            "The coefficient between feature 3504 and feature 3505 is -0.0029108144129601373\n",
            "The coefficient between feature 2978 and feature 2979 is -0.0005002501250625338\n",
            "The coefficient between feature 3562 and feature 3563 is -0.000704766164423331\n",
            "The coefficient between feature 2489 and feature 2490 is -0.001985910975299354\n",
            "The coefficient between feature 3042 and feature 3043 is -0.0005002501250625304\n",
            "The coefficient between feature 2124 and feature 2125 is -0.0028681989396153874\n",
            "The coefficient between feature 1354 and feature 1355 is -0.0017247144648137164\n",
            "The coefficient between feature 3313 and feature 3314 is -0.0005002501250625279\n",
            "The coefficient between feature 3147 and feature 3148 is -0.002609910907928634\n",
            "The coefficient between feature 3066 and feature 3067 is -0.0064532828480759604\n",
            "The coefficient between feature 3429 and feature 3430 is -0.0005002501250625308\n",
            "The coefficient between feature 2762 and feature 2763 is -0.0007075393047977633\n",
            "The coefficient between feature 3519 and feature 3520 is -0.005324032897367589\n",
            "The coefficient between feature 1383 and feature 1384 is -0.001349244433176325\n",
            "The coefficient between feature 2161 and feature 2162 is -0.004727722714698516\n",
            "The coefficient between feature 95 and feature 96 is -0.0005002501250625292\n",
            "The coefficient between feature 954 and feature 955 is 0.24300401167528346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qdDoVzmRAA1",
        "outputId": "dc0b11b4-e537-415b-b34e-b1bd984f17e8"
      },
      "source": [
        "print(\"After PCA, we reduced the dimensionality of the training data to\", x_train.shape[1])\n",
        "print(\"After PCA, we reduced the dimensionality of the testing data to\", x_test.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After PCA, we reduced the dimensionality of the training data to 1316\n",
            "After PCA, we reduced the dimensionality of the testing data to 1316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsfmQlBiPObJ",
        "outputId": "54f2b11a-f736-4cba-bc0a-5f8e4a127c87"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# feature values are not normally distributed but trying the model\n",
        "gaussian_model = GaussianNB()\n",
        "gaussian_model.fit(x_train, y_train)\n",
        "prediction = gaussian_model.predict(x_test)\n",
        "accuracy = accuracy_score(prediction, y_test)\n",
        "print(\"Gaussian Naive Bayes produced an accuracy score of\", accuracy, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes produced an accuracy score of 0.5325 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqEhfeM2TdRg",
        "outputId": "6499fddf-0c5e-4eb1-f88b-8a2e4ad3c497"
      },
      "source": [
        "## Using K Means Clustering from sklearn with PCA\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans_model = KMeans(n_clusters = 2)\n",
        "kmeans_model.fit(x_train,y_train)\n",
        "prediction = kmeans_model.predict(x_test)\n",
        "accuracy = accuracy_score(prediction, y_test)\n",
        "print(\"KMeans produced an accuracy score of\", accuracy, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KMeans produced an accuracy score of 0.6625 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ0QdXsVTgXR",
        "outputId": "d761e434-e417-4efc-c1d9-0b565485e00a"
      },
      "source": [
        "## Using SVM from sklearn with PCA\n",
        "svm_model = svm.SVC(kernel = 'rbf', gamma = 'scale')\n",
        "\n",
        "svm_model.fit(x_train, y_train)\n",
        "\n",
        "score = svm_model.score(x_test, y_test)\n",
        "\n",
        "print(\"SVM produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM produced an accuracy score of 0.84 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmApyOD7TjcS",
        "outputId": "b4693f63-d639-443c-e5e1-9719905c5f74"
      },
      "source": [
        "## Using Logistic Regression without regularization with PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logit_model = LogisticRegression(penalty = 'none').fit(x_train, y_train)\n",
        "score = logit_model.score(x_test, y_test)\n",
        "print(\"Logistic regression without penalty produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression without penalty produced an accuracy score of 0.7875 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY_bf8IuTosf",
        "outputId": "545f86a5-9177-43a6-a002-5ce99637a9d7"
      },
      "source": [
        "## Using Logistic Regression with L2 penalty wiht PCA\n",
        "\n",
        "logit_model_2= LogisticRegression(penalty = 'l2').fit(x_train, y_train)\n",
        "score = logit_model_2.score(x_test, y_test)\n",
        "print(\"Logistic regression produced an accuracy score of\", score, \"on the test set\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression produced an accuracy score of 0.83 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0YNZqJvT08F"
      },
      "source": [
        "## **TFIDF Scores, Data Scaling and LDA for classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUrh9ZLGT-Wh",
        "outputId": "1ef5029b-d498-4e07-c68f-514395a9a1b4"
      },
      "source": [
        "## Use LDA to reduce dimensionality to 2\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "tfidfvector = TfidfVectorizer(max_features = 5000)\n",
        "tfidf_scores = tfidfvector.fit_transform(selected['Text_Final'])\n",
        "\n",
        "## Scare the entire tfidf score vector \n",
        "scaled_scores = MinMaxScaler().fit_transform(tfidf_scores.toarray())\n",
        "\n",
        "## train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(scaled_scores, selected[['Recommended IND']], test_size = 0.2 )\n",
        "\n",
        "## fit in LDA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "lda = LDA(n_components = None)\n",
        "lda.fit(scaled_scores, selected['Recommended IND'].values)\n",
        "x_train_lda = lda.transform(x_train)                                                                                                              \n",
        "x_test_lda = lda.transform(x_test)\n",
        "\n",
        "## Use lda to predict \n",
        "predictions = lda.predict(x_test)\n",
        "accuracy = accuracy_score(predictions, y_test)\n",
        "print(\"LDA produced an accuracy score of\", accuracy, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDA produced an accuracy score of 0.9925 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V_1iRMVBZQd"
      },
      "source": [
        "## **Bag of Words Vectorization, no scaling, no dimension reduction** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRwu8HnTBjy_"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train,y_test = train_test_split(selected['Text_Final'], selected['Recommended IND'], test_size = 0.2,\n",
        "                                                   random_state = 28)\n",
        "\n",
        "bow_model = CountVectorizer(max_features=5000)\n",
        "bow_model.fit(selected['Text_Final'])\n",
        "x_train_transformed = bow_model.transform(x_train)\n",
        "x_test_transformed = bow_model.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QmAD_c-FY_Q",
        "outputId": "8bd02a89-39ad-429f-c127-28db4db74510"
      },
      "source": [
        "## Using Multinomial Naive Bayes\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "bayes_model = naive_bayes.MultinomialNB()\n",
        "bayes_model.fit(x_train_transformed, y_train)\n",
        "# predict the labels on validation dataset\n",
        "predictions = bayes_model.predict(x_test_transformed)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "accuracy = accuracy_score(predictions, y_test)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Accuracy Score is \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes Accuracy Score is  0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDk_2g09GYt2",
        "outputId": "e92915fc-8f04-4152-d3f0-655ba72c0629"
      },
      "source": [
        "## Using Logistic Regression \n",
        "logit_model = LogisticRegression()\n",
        "logit_model.fit(x_train_transformed, y_train)\n",
        "score = logit_model.score(x_test_transformed, y_test)\n",
        "print(\"Logistic Regression accuracy scroe is\", score )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression accuracy scroe is 0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Mppt9C8d2I",
        "outputId": "bcd50394-35c6-4219-ff05-a0550da26556"
      },
      "source": [
        "## Using Logistic Regression without penalty\n",
        "logit_model = LogisticRegression(penalty = 'none')\n",
        "logit_model.fit(x_train_transformed, y_train)\n",
        "score = logit_model.score(x_test_transformed, y_test)\n",
        "print(\"Logistic Regression accuracy scroe is\", score )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression accuracy scroe is 0.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D92dbvQhGyh5",
        "outputId": "f52381f7-cfa1-4117-8e36-098fef4045c2"
      },
      "source": [
        "## Using K Means \n",
        "\n",
        "kmeans_model = KMeans(n_clusters = 2)\n",
        "kmeans_model.fit(x_train_transformed,y_train)\n",
        "prediction = kmeans_model.predict(x_test_transformed)\n",
        "accuracy = accuracy_score(prediction, y_test)\n",
        "print(\"KMeans produced an accuracy score of\", accuracy, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KMeans produced an accuracy score of 0.53 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG-X7xNLHLZv",
        "outputId": "558b701b-625f-4ae3-dc52-6593e79c1078"
      },
      "source": [
        "## Using SVM\n",
        "svm_model = svm.SVC(kernel = 'rbf', gamma = 'scale')\n",
        "\n",
        "svm_model.fit(x_train_transformed, y_train)\n",
        "\n",
        "score = svm_model.score(x_test_transformed, y_test)\n",
        "\n",
        "print(\"SVM produced an accuracy score of\", score, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM produced an accuracy score of 0.8175 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rgkp0pv8mR2",
        "outputId": "e453f327-f4ec-4ebc-9664-6eaa87868a16"
      },
      "source": [
        "## Using in LDA\n",
        "\n",
        "lda = LDA(n_components = None)\n",
        "lda.fit(x_train_transformed.toarray(), y_train)\n",
        "\n",
        "## Use lda to predict \n",
        "predictions = lda.predict(x_test_transformed)\n",
        "accuracy = accuracy_score(predictions, y_test)\n",
        "print(\"LDA produced an accuracy score of\", accuracy, \"on the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDA produced an accuracy score of 0.645 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJeqpG8Pa7Im"
      },
      "source": [
        "## **RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXO-P5sxaojT"
      },
      "source": [
        "## Create a word list for all the left words in the cleaned reviews to be used for encoding\n",
        "\n",
        "## Organize the words by its frequency from large to small\n",
        "\n",
        "word_list = []\n",
        "for review in selected['Cleaned_Text']:\n",
        "    for word in review:\n",
        "        word_list.append(str(word))\n",
        "        \n",
        "#create a Counter object to map all the words to an integer value which is their frequency, sorted from largest to smallest\n",
        "# (code took from https://medium.com/@lamiae.hana/a-step-by-step-guide-on-sentiment-analysis-with-rnn-and-lstm-3a293817e314)\n",
        "from collections import Counter\n",
        "vocab = Counter(word_list)\n",
        "vocab = sorted(vocab, key = vocab.get, reverse= True)\n",
        "\n",
        "# Assign an integer to each word, the integer is the word index plus 1\n",
        "vocab_int = {word: index  for index, word in enumerate(vocab,1)}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "84DvZJ8BbL-p",
        "outputId": "7c34acf0-2038-491e-d772-2264537121f7"
      },
      "source": [
        "## Map each review to a list of integers based on the vocabulary map  \n",
        "## Initialize a list to store all the reviews as a list of integers\n",
        "\n",
        "integer_list = []\n",
        "\n",
        "for i in range(len(selected['Cleaned_Text'])):\n",
        "    review = selected['Cleaned_Text'][i]\n",
        "    review_int = []\n",
        "    for word in review:\n",
        "        review_int.append(vocab_int[word])\n",
        "\n",
        "    integer_list.append(review_int)\n",
        "    \n",
        "selected['Review_Int'] = integer_list\n",
        "\n",
        "selected.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review Text</th>\n",
              "      <th>Recommended IND</th>\n",
              "      <th>Cleaned_Text</th>\n",
              "      <th>Text_Final</th>\n",
              "      <th>Review_Int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Great layering top. so comfy, feels soft as my...</td>\n",
              "      <td>1</td>\n",
              "      <td>[great, layer, top, comfy, feel, soft, pjs, bu...</td>\n",
              "      <td>['great', 'layer', 'top', 'comfy', 'feel', 'so...</td>\n",
              "      <td>[15, 212, 8, 194, 61, 46, 988, 22, 9]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As much as i wanted to love this top, it was t...</td>\n",
              "      <td>0</td>\n",
              "      <td>[much, want, love, top, flowy, felt, like, loo...</td>\n",
              "      <td>['much', 'want', 'love', 'top', 'flowy', 'felt...</td>\n",
              "      <td>[36, 35, 5, 8, 230, 141, 6, 2, 50, 11, 3, 176,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I ordered my usual size and i think the length...</td>\n",
              "      <td>0</td>\n",
              "      <td>[order, usual, size, think, length, top, chest...</td>\n",
              "      <td>['order', 'usual', 'size', 'think', 'length', ...</td>\n",
              "      <td>[14, 178, 3, 24, 55, 8, 2240, 15, 73, 82, 305,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When i unwrapped this a chemical \"stink\" hit m...</td>\n",
              "      <td>0</td>\n",
              "      <td>[unwrap, chemical, stink, hit, amost, knock, r...</td>\n",
              "      <td>['unwrap', 'chemical', 'stink', 'hit', 'amost'...</td>\n",
              "      <td>[1680, 1681, 1682, 174, 2241, 1400, 19, 2242, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As others have said, this is cropped, so will ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[others, say, crop, look, funny, pant, arent, ...</td>\n",
              "      <td>['others', 'say', 'crop', 'look', 'funny', 'pa...</td>\n",
              "      <td>[456, 94, 393, 2, 618, 54, 844, 116, 699, 11, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Review Text  ...                                         Review_Int\n",
              "0  Great layering top. so comfy, feels soft as my...  ...              [15, 212, 8, 194, 61, 46, 988, 22, 9]\n",
              "1  As much as i wanted to love this top, it was t...  ...  [36, 35, 5, 8, 230, 141, 6, 2, 50, 11, 3, 176,...\n",
              "2  I ordered my usual size and i think the length...  ...  [14, 178, 3, 24, 55, 8, 2240, 15, 73, 82, 305,...\n",
              "3  When i unwrapped this a chemical \"stink\" hit m...  ...  [1680, 1681, 1682, 174, 2241, 1400, 19, 2242, ...\n",
              "4  As others have said, this is cropped, so will ...  ...  [456, 94, 393, 2, 618, 54, 844, 116, 699, 11, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "882D5oQBbogJ"
      },
      "source": [
        "## Pad sequence\n",
        "\n",
        "# use pad_sequence function from Keras to pad each review to be of a length of the review with the maximum length of all the reviews \n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "print('The longest review in the data has', max(len(review) for review in selected['Review_Int']), 'words')\n",
        "\n",
        "# set maximum number of words \n",
        "max_words = max(len(review) for review in selected['Review_Int'])\n",
        "padded_reviews = sequence.pad_sequences(selected['Review_Int'], maxlen = max_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQACF3WacJOS"
      },
      "source": [
        "# split the data into training and testing data\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_reviews, selected['Recommended IND'].values, test_size = 0.2, random_state =6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQjMGSdpb1sY",
        "outputId": "92c4a159-ee11-4c7a-e676-a86d45c2efd5"
      },
      "source": [
        "## Build a RNN (code took from https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e)\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model_rnn = Sequential()\n",
        "\n",
        "embedding_size = 8\n",
        "model_rnn.add(Embedding(5000, embedding_size, input_length=max_words))\n",
        "model_rnn.add(LSTM(100))\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_rnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'] )\n",
        "\n",
        "model_rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 58, 8)             40000     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               43600     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 83,701\n",
            "Trainable params: 83,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AitLgf7zckXS",
        "outputId": "a9f4ad01-dd00-463c-c1d9-eb12a7136e5f"
      },
      "source": [
        "history = model_rnn.fit(x_train, y_train, epochs = 10, validation_data = (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 0.0100 - accuracy: 0.9994 - val_loss: 0.7497 - val_accuracy: 0.8125\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 2s 42ms/step - loss: 0.0149 - accuracy: 0.9956 - val_loss: 0.8825 - val_accuracy: 0.7875\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 2s 43ms/step - loss: 0.0174 - accuracy: 0.9975 - val_loss: 0.7983 - val_accuracy: 0.7975\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 2s 42ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.8786 - val_accuracy: 0.8050\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 2s 43ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.9341 - val_accuracy: 0.8025\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 2s 43ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9518 - val_accuracy: 0.8025\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 2s 41ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9958 - val_accuracy: 0.8075\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.0300 - val_accuracy: 0.8025\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 2s 41ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.0402 - val_accuracy: 0.8025\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.0719 - val_accuracy: 0.8100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "Vjwf0DVXdPEo",
        "outputId": "c466b80f-e032-4f07-e242-4fbd859a8e12"
      },
      "source": [
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f289bacfa58>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZFfGWkHcPeg",
        "outputId": "c7f16a08-42d5-4004-9bf2-d613a953b1ec"
      },
      "source": [
        "scores = model_rnn.evaluate(x_test, y_test)\n",
        "print('RNN accuracy score on the test set is', scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 13ms/step - loss: 1.0719 - accuracy: 0.8100\n",
            "RNN accuracy score on the test set is 0.8100000023841858\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}